{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinHash & LSH \n",
    "Okay, so I think I will understand this better if I implement the various pieces of the algorithm/s in python first, and then figure out how they will work in MyriaL. \n",
    "\n",
    "From p. 91 of the MMDS book, we have these steps for combining min-hash and LSH. I switched the book's variables to the ones we have been using, specifically:\n",
    "\n",
    "    n: as in n-gram\n",
    "    m: # of groups of k hash fns\n",
    "    k: # of hash functions in a group\n",
    "    l: length of minhash signatures\n",
    "    t: threshold\n",
    "\n",
    "1. Pick a value of n and construct from each document the set of n-shingles.\n",
    "2. Sort the document-shingle pairs to order them by shingle.\n",
    "3. Pick a length l for the minhash signatures. Feed the sorted list to the algorithm of Section 3.3.5 to compute the minhash signatures for all the documents.\n",
    "4. Choose a threshold t that defines how similar documents have to be in order for them to be regarded as a desired “similar pair.” Pick a number of bands m and a number of rows k such that mk = l, and the threshold t is approximately (1/m)1/k. If avoidance of false negatives is important, you may wish to select m and r to produce a threshold lower than t; if speed is important and you wish to limit false positives, select m and k to produce a higher threshold.\n",
    "5. Construct candidate pairs by applying the LSH technique of Section 3.4.1.\n",
    "6. Examine each candidate pair’s signatures and determine whether the fraction of components in which they agree is at least t.\n",
    "7. Optionally, if the signatures are sufficiently similar, go to the documents themselves and check that they are truly similar, rather than documents that, by luck, had similar signatures.\n",
    "\n",
    "I will break this computation down by the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the cora data\n",
    "cora_data = pd.DataFrame.from_csv(\"../data/cora/cora.txt\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Pick a value of n and compute n-grams\n",
    "`n=5` for now\n",
    "\n",
    "According to the MMDS book, `n=5` should work, except common letters skew this for large documents, and `n=9` is considered safe for large documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "def find_ngrams(input_list, n):\n",
    "  return zip(*[input_list[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute all n-grams for the cora data\n",
    "cora_data.fillna(\"\", inplace=True)\n",
    "cora_data['ngram_author'] = cora_data[2].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_volume'] = cora_data[3].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_title'] = cora_data[4].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_institute'] = cora_data[5].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_venue'] = cora_data[6].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_address'] = cora_data[7].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_pub'] = cora_data[8].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_year'] = cora_data[9].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_pages'] = cora_data[10].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_editor'] = cora_data[11].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_note'] = cora_data[12].apply(find_ngrams, n=5)\n",
    "cora_data['ngram_month'] = cora_data[13].apply(find_ngrams, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append all the n-grams of the cora data\n",
    "cora_ngrams = []\n",
    "for record in cora_data.ix[:,14:].values.tolist():\n",
    "    cora_ngrams.append([item for sublist in record for item in sublist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Order the records by ngram\n",
    "## Is this the correct interpretation? Probably not...\n",
    "\"Sort the document-shingle pairs to order them by shingle.\" --> Order the records by ngram\n",
    "\n",
    "Do we need to sort the characters within the ngrams?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cora_ngrams_sorted = sorted(cora_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pick a length l for minhash signatures & compute minhash\n",
    "\n",
    "Oooookay this is a big step.\n",
    "\n",
    "`l` is the number of times we permute the rows (records?), so I guess `l` should approximately equal the alphabet size? This intuition could be way off. If we include lower case letters, upper case letters, digits, punctuation, & special characters, there are 95 ascii characters.\n",
    "\n",
    "using `l=95`\n",
    "\n",
    "\n",
    "Resources: \n",
    "\n",
    "* Section 3.3.5 in MMDS\n",
    "\n",
    "* https://rajmak.wordpress.com/2014/12/22/locality-sensitive-hashing-lsh-map-reduce-in-python/\n",
    "\n",
    "* Copied from the book from Dan p.1115 (variables not changed to what we've been using):\n",
    "\n",
    "      FOR i := 1 TO k DO\n",
    "          FOR j := 1 TO m DO\n",
    "              Vij := inf;\n",
    "      FOR EACH basket b DO BEGIN\n",
    "          FOR j := 1 TO m DO \n",
    "              compute hj(b);\n",
    "          FOR EACH item i in b DO \n",
    "              FOR j := 1 TO m DO\n",
    "                  IF hj(b)<Vij THEN Vij := hj(b);\n",
    "      END\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Choose a threshold, pick k & m accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Apply LSH to pick candidate pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Compute P(A=B) from the minhash signature\n",
    "\"Examine each candidate pair’s signatures and determine whether the fraction of components in which they agree is at least t.\" --> Is this effectively computing P(A=B) from the minhash signature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Compute J(A, B) on the pairs that pass P(A=B)>t if we care about false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
